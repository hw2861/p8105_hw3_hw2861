---
title: "p8105_hw3_hw2861"
author: "Hongmiao Wang"
date: "2022-10-09"
output: github_document
---


```{r setup}
library(tidyverse)
library(p8105.datasets)
library(patchwork)


theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

 
```


### Problem 1
### Question 1 is not scored and comes from the answers already given.

#### Read in the data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```
#### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.


```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```


Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```



## Question 2

First, i will Load and tidy the **Accelerometer** data.
```{r Accelerometer}
Accelerometer = read_csv("Dataset/accel_data.csv") %>%
janitor::clean_names() %>% 
mutate(weekday_vs_weekend = recode(day,"Monday" = "Weekday", 
 "Tuesday"= "Weekday", "Wednesday" = "Weekday", "Thursday" = "Weekday",
 "Friday" = "Weekday", "Saturday" = "Weekend", "Sunday" = "Weekend", )) %>%
select(week, day_id, day, weekday_vs_weekend, everything())%>%
pivot_longer(
activity_1:activity_1440,
names_prefix = "activity_",
names_to = "activity_minute",
values_to = "count")


```

The resulting dataset contains accelerometer data for an 63 year-old male over a five-week period. In the resulting dataset **Accelerometer** , there are **`r nrow(Accelerometer)`** observations and **`r ncol(Accelerometer)`** variables. These variables are **`r names(Accelerometer)`**.



I aggregate across minutes to create a total activity variable for each day
```{r Accel_everyday table}
Accel_everyday = Accelerometer %>% 
  group_by(day_id,week,day)%>% 
  summarize(day_activity= sum(count))

knitr::kable(Accel_everyday)
```

Based on the table：

Activity counts at the weekends are generally higher than Activity counts on the weekdays in the first 2 weeks.But On the last two Saturdays during week4 and week5 (Day24 and Day31), the patients' activity counts decreased significantly.



Make a single-panel plot that shows the 24-hour activity time courses for each day
```{r plot}
Accelerometer %>% 
  mutate(day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday","Thursday", "Friday","Saturday", "Sunday")))%>%
  group_by(day_id) %>%
  mutate(activity_minute=as.numeric(activity_minute))%>%
  ggplot(aes(x = activity_minute, y = count, color = day)) + 
  geom_point() + geom_line() + 
  labs(
    x = "Time",
    y = "Activity Counts",
    title = "The 24-hour activity time courses for each day") + 
  scale_x_continuous(
    limits = c(0, 1440),
    breaks = c(0, 360, 720, 1080, 1440), 
    labels = c("12am","6am", "12pm", "6pm", "12am"))+ 
   theme(legend.position = "right")
```

Based on this graph:

Activity counts are generally lower between midnight and 6am. Also, the activity counts of the patient are lower than 2500 for the majority of the time. It appears that the high activity counts are usually from 8pm to 10pm, especially on Fridays. Also, around 12pm on Sunday, activity counts also increased compared to the usual.



## Question 3

### Short description of the dataset

Load the **ny_noaa** dataset
```{r ny_noaa}
data("ny_noaa")
```

The **ny_noaa** dataset is about the weather data for all New York state weather stations between  **`r min(ny_noaa$date)`**  and  **`r max(ny_noaa$date)`**.
The **ny_noaa** dataset contains **`r ncol(ny_noaa)`** columns(variables) and **`r nrow(ny_noaa)`** rows(observations).

These variables in this dataset are **`r names(ny_noaa)`**. Two of the key variables are the id and date, which tells us about the id of Weather station and the date of observation. With the key variables tmax and tmin, we can know the Maximum temperature and Minimum temperature of the day. The variable like prcp gives us the information about the Precipitation. The two variables like snow and snwd give us the information about snowfall and snowdepth in mm.


```{r Percentage_missing_data}
colMeans(is.na(ny_noaa))%>%
knitr::kable(col.names = c('Percentage_miss'))
```

For the variables **prcp**, there are **`r sum(is.na(ny_noaa$prcp))`** missing values in total. And the percentage of missing values in variables **prcp** is  **5.62%** .
For the variables **snow**, there are **`r sum(is.na(ny_noaa$snow))`** missing values in total. And the percentage of missing values in variables **snow** is  **14.69%** .
For the variables **snwd**, there are **`r sum(is.na(ny_noaa$snwd))`** missing values in total. And the percentage of missing values in variables **snwd** is  **22.80%** .
For the variables **tmax**, there are **`r sum(is.na(ny_noaa$tmax))`** missing values in total. And the percentage of missing values in variables **tmax** is  **43.71%** .
For the variables **tmin**, there are **`r sum(is.na(ny_noaa$tmin))`** missing values in total. And the percentage of missing values in variables **tmin** is also  **43.71%** .


### Part1 in Q3
Do some data cleaning.
```{r clean }
ny_noaa_clean = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"),sep ="-",convert = TRUE)%>% 
    mutate(
    prcp = prcp*0.1,
    tmax = as.numeric(tmax)*0.1,
    tmin = as.numeric(tmin)*0.1)
```

* After cleaning, I Created separate variables for year, month, and day. 
* After cleaning, the unit of precipitation has been change into mm. The unit of tmax and tmin has been change into degrees C.

```{r counting things}
ny_noaa_clean %>%
  count(snow, name = "n_obs")%>%
  arrange(desc(n_obs))
```
For snowfall, The most commonly observed values is **0**. This commonly observed value is in line with expectations. As it does not snow on most days in New York. For the vast majority of days in summer,autumn and spring, the snowfall should be 0.


### Part2 in Q3
Make a two-panel plot showing the average max temperature in January and in July in each station across years.
```{r two-panel plot}
 ny_noaa_clean %>% 
  filter(month == c(1, 7)) %>%
  group_by(id,year,month) %>% 
  summarize(temp_mean_max = mean(tmax),na.rm = TRUE) %>% 
  ggplot(aes(x = year, y = temp_mean_max, color=as.factor(month))) + 
  geom_point() + geom_smooth()+
    labs(
    title = "Average max Temp in Jan and Jul in each station across years",
    x = "Year",
    y = "Average max temperature(degrees C)") +
 scale_x_continuous(
    limits = c(1980, 2010),
    breaks = c(1980, 1985, 1990, 1995,2000,2005,2010), 
    labels = c("1980","1985","1990","1995", "2000", "2005","2010")
    ) +
  facet_grid(. ~ month) +
  theme(legend.position = "none") +
  scale_color_manual(values = c("blue", "orange")) 
 
```

Based on the two-panel plot,
We can observe that the average maximum temperature in January is usually in the range of -10°C to 10°C, with the mean of (average maximum temperature) around 0°C. There were some outliers in January 1982, when they had an average max temperature around -10°C. Also, outliers were seen in January 2008, at around -5°C.

We can observe that the average maximum temperature in July is usually in the range of 20°C to 30°C, with the mean around 27°C. There are more outliers for average max temperatures in July than in January. The most obvious one appeared in July 1988, a weather station reported an average maximum temperature of around 14°C for that month. The other obvious outlier appeared in July 2010, a weather station reported an average maximum temperature of around 33°C for that month.

Based on the line generated from geom_smooth, We can tell that the average maximum temperature across years fluctuates more drastically in January than in July.


### Part3 in Q3
(i) tmax vs tmin for the full dataset
```{r tmax vs tmin}
noaa_tmax_tmin=ny_noaa_clean %>% 
  filter(!is.na(tmin), !is.na(tmax))%>% 
  ggplot(aes(x = tmin, y = tmax)) + 
    geom_hex()+
    labs(
    title = "The relationship between tmin and tmax",
    x = "Minimum temperature (degrees C)",
    y = "Maximum temperature (degrees C)") +
   theme(legend.position = "right")

noaa_tmax_tmin
```

(ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r  the distribution of snowfall values}
noaa_snowfall = ny_noaa_clean %>% 
  mutate(year = factor(year)) %>%
   filter(0 < snow, snow < 100) %>%
  ggplot(aes(x = year, y = snow)) + 
  geom_violin() + 
    labs(
    title = "The distribution of snowfall values (0-100mm) by years",
    x = "Year",
    y = "Snowfall (mm)")+
  theme(axis.text.x = element_text(angle = 90))
   
noaa_snowfall 

```

(iii) patchwork
```{r  patchwork}
noaa_tmax_tmin / noaa_snowfall

```

* The above plot shows tmax vs tmin for the full dataset.
* The blow plot shows the distribution of snowfall values greater than 0 and less than 100 separately by year.